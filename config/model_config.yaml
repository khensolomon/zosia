# config/model_config.yaml

# Transformer Model Hyperparameters
model_type: "Transformer" # Can be extended for other model types (e.g., "GRU", "LSTM")

# Encoder parameters
enc_layers: 6
enc_heads: 8
enc_pf_dim: 2048 # Position-wise Feedforward Dimension
enc_dropout: 0.1

# Decoder parameters
dec_layers: 6
dec_heads: 8
dec_pf_dim: 2048
dec_dropout: 0.1

# Shared parameters
hidden_dim: 512 # Dimension of embeddings and sub-layer outputs
max_seq_len: 128 # Must match max_sequence_length in data_config.yaml
# vocab_size will be dynamically loaded from the tokenizer/vocab