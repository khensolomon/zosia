# config/training_config.yaml

# Training Parameters
epochs: 5 #For your very small sample data, you might want to set this to a lower number like 5 or 10 initially. Training on very little data for many epochs won't yield good results and just wastes time.
batch_size: 64
learning_rate: 0.0005
optimizer: "AdamW" # AdamW, SGD, etc.
scheduler: "Noam"  # Noam, WarmupLinear, StepLR, None
warmup_steps: 4000 # For Noam scheduler
gradient_clip: 1.0 # Max gradient norm for clipping
mixed_precision: True # Use AMP (Automatic Mixed Precision) for faster training on GPUs
accumulate_grad_batches: 1 # Number of batches to accumulate gradients over

# Evaluation and Logging
eval_interval_epochs: 1 # Evaluate on validation set every N epochs
log_interval_steps: 100 # Log training metrics every N steps
save_interval_epochs: 5 # Save model checkpoint every N epochs
save_best_model: True # Save only the model with the best validation BLEU score

# Experiment Management
experiment_name_prefix: "nmt_run" # Prefix for experiment folder in 'experiments/'
# Use wandb: True/False to enable/disable Weights & Biases logging
use_wandb: True
wandb_project_name: "ZoSia_NMT"
wandb_entity: "your_wandb_username" # Replace with your Weights & Biases username