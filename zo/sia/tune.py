"""
Hyperparameter Tuning Script for Zolai-NMT
version: 2025.08.04.173000

This script uses the Optuna library to automatically find the best
hyperparameters for the NMT model. It relies on a pre-built data index
generated by `preprocess.py`.
"""
import os
import sys
import argparse
import yaml
import logging
import optuna
from tqdm import tqdm
import torch
import warnings

# Safely ignore the specific PyTorch UserWarning about nested tensors.
warnings.filterwarnings("ignore", message=".*nested tensors.*", category=UserWarning)

# Add the parent directory to the path to allow sibling imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from zo.sia.main import (
    run_training_session,
    Tokenizer,
    Config
)

# --- (The rest of the script is unchanged) ---

optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))

def objective(trial, args, params, device, tokenizer):
    trial_params = params.copy()
    trial_params.update({
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
        'num_layers': trial.suggest_int('num_layers', 2, 4),
        'embedding_size': trial.suggest_categorical('embedding_size', [128, 256]),
        'num_heads': trial.suggest_categorical('num_heads', [4, 8]),
        'ff_hidden_size': trial.suggest_categorical('ff_hidden_size', [256, 512]),
        'dropout': trial.suggest_float('dropout', 0.1, 0.3)
    })
    best_validation_loss = run_training_session(trial_params, args, device, tokenizer)
    return best_validation_loss

def main():
    parser = argparse.ArgumentParser(description="Hyperparameter Tuning for Zolai-NMT")
    subparsers = parser.add_subparsers(dest="command", required=True)

    run_parser = subparsers.add_parser('run', help="Run a new tuning study.")
    run_parser.add_argument('--source', type=str, required=True, choices=Config.SUPPORTED_LANGUAGES)
    run_parser.add_argument('--target', type=str, required=True, choices=Config.SUPPORTED_LANGUAGES)
    run_parser.add_argument('--effort', type=str, default='medium', choices=['quick', 'medium', 'thorough'],
                              help="The level of effort for the study.")
    run_parser.add_argument('--trials', type=int, default=None,
                              help="Manually specify the number of trials, overriding --effort.")

    show_parser = subparsers.add_parser('show-best', help="Show the best results from a previous study.")
    show_parser.add_argument('--source', type=str, required=True, choices=Config.SUPPORTED_LANGUAGES)
    show_parser.add_argument('--target', type=str, required=True, choices=Config.SUPPORTED_LANGUAGES)

    args = parser.parse_args()

    if args.command == 'run':
        effort_map = {'quick': 10, 'medium': 50, 'thorough': 200}
        n_trials = args.trials if args.trials is not None else effort_map[args.effort]

        train_args = argparse.Namespace(
            source=args.source,
            target=args.target,
            resume=False
        )
        
        with open(Config.DEFAULT_HYPERPARAMS_FILE, 'r') as f:
            base_params = yaml.safe_load(f)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        lang_pair_sorted = sorted([args.source, args.target])
        tokenizer_path = os.path.join(Config.EXPERIMENTS_DIR, Config.TOKENIZER_PREFIX_PATTERN.format(src=lang_pair_sorted[0], tgt=lang_pair_sorted[1]) + ".model")
        if not os.path.exists(tokenizer_path):
            sys.exit(f"Error: Tokenizer not found at '{tokenizer_path}'. Please run the preprocessing script first.")
        tokenizer = Tokenizer(tokenizer_path)

        os.makedirs(Config.EXPERIMENTS_DIR, exist_ok=True)
        storage_path = f"sqlite:///{os.path.join(Config.EXPERIMENTS_DIR, 'tuning_studies.db')}"
        study_name = f"{lang_pair_sorted[0]}-{lang_pair_sorted[1]}-tuning"

        study = optuna.create_study(
            study_name=study_name,
            storage=storage_path,
            load_if_exists=True,
            direction='minimize'
        )
        
        completed_trials = len(study.trials)
        if completed_trials >= n_trials:
            print(f"Study '{study_name}' already has {completed_trials} trials. Target is {n_trials}.")
            print("To run more trials, increase the --effort or --trials value.")
            return
            
        remaining_trials = n_trials - completed_trials
        print(f"Resuming study '{study_name}'. {completed_trials} trials complete, {remaining_trials} remaining.")

        with tqdm(total=remaining_trials, desc="Tuning Progress") as pbar:
            def callback(study, trial):
                pbar.update(1)

            study.optimize(lambda trial: objective(trial, train_args, base_params, device, tokenizer), n_trials=remaining_trials, callbacks=[callback])

        print("\n--- Tuning Complete ---")
        print(f"Best validation loss: {study.best_value}")
        print("Best hyperparameters found:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")

        output_path = Config.HYPERPARAMS_OUTPUT_PATTERN.format(src=args.source, tgt=args.target)
        best_tuned_params = {k: v for k, v in study.best_params.items() if k in [
            'learning_rate', 'num_layers', 'embedding_size', 'num_heads', 'ff_hidden_size', 'dropout'
        ]}
        
        os.makedirs(Config.CONFIG_DIR, exist_ok=True)
        with open(output_path, 'w') as f:
            yaml.dump(best_tuned_params, f, default_flow_style=False)
        
        print(f"\nBest parameters saved to: {output_path}")

    elif args.command == 'show-best':
        path = Config.HYPERPARAMS_OUTPUT_PATTERN.format(src=args.source, tgt=args.target)
        if not os.path.exists(path):
            print(f"Error: No tuned hyperparameter file found at '{path}'.")
            print("Please run a study first using the 'run' command.")
            sys.exit(1)
        
        with open(path, 'r') as f:
            best_params = yaml.safe_load(f)
        
        print(f"--- Best Found Hyperparameters for {args.source}-{args.target} ---")
        for key, value in best_params.items():
            print(f"  {key}: {value}")

if __name__ == '__main__':
    main()
